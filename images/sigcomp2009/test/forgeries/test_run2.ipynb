{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D, concatenate, Activation\n",
    "from keras.applications.mobilenet import MobileNet as Net\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "ALPHA = 0.3  # Triplet Loss Parameter\n",
    "\n",
    "from keras.layers import Input,Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D,concatenate,Activation\n",
    "from keras.applications.xception import Xception as Net\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "import pydot\n",
    "import graphviz\n",
    "import time\n",
    "import glob\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(x):\n",
    "    anchor, positive, negative = x\n",
    "    \n",
    "    #x = tf.constant([[1, 1, 1], [1, 1, 1]]) ------ tf.reduce_sum(x, 1)  # [3, 3]\n",
    "    #It is trained in batches so, the tensor will be of the above shape for each triplet\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
    "    \n",
    "    #x = tf.constant([[1, 1, 1], [1, 1, 1]])--------->tf.reduce_sum(x, 0)  # [2, 2, 2]\n",
    "\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape):\n",
    "    \n",
    "    cnn_model = Sequential()\n",
    "    cnn_model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    cnn_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    cnn_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(d1, d2, c):\n",
    "    \n",
    "    # The triplet network takes 3 input images: 2 of the same class and 1 out-of-class sample\n",
    "    #shape: A shape tuple (integers), not including the batch size. For instance, shape=(32,) \n",
    "    #indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "    \n",
    "    anchor_example =   Input(shape=(d1, d2, c), name = 'anchor')\n",
    "    positive_example = Input(shape=(d1, d2, c), name = 'positive')\n",
    "    negative_example = Input(shape=(d1, d2, c), name = 'negative')\n",
    "        \n",
    "    #base_model = Net(input_shape=(d1,d2,3),weights='imagenet',include_top=False)\n",
    "    base_model = make_model(input_shape=(d1, d2, c))\n",
    "    # the weights of this layer will be set to ones and fixed  (since they\n",
    "    # are shared we could also leave them trainable to get a weighted sum)\n",
    "    \n",
    "    # feed all 3 inputs into the pretrained keras model\n",
    "    x1 = base_model(anchor_example)\n",
    "    x2 = base_model(positive_example)\n",
    "    x3 = base_model(negative_example)\n",
    "    \n",
    "    # flatten/summarize the models output:\n",
    "    # (here we could also use GlobalAveragePooling or simply Flatten everything)\n",
    "    #Takes the max value for each of the filter\n",
    "    anchor =   GlobalMaxPooling2D()(x1)\n",
    "    positive = GlobalMaxPooling2D()(x2)\n",
    "    negative = GlobalMaxPooling2D()(x3)\n",
    "    \n",
    "    \n",
    "    #Loss gives the final loss value between a,p and n.\n",
    "    loss = merge([anchor, positive, negative], mode=triplet_loss, output_shape=(1,))\n",
    "\n",
    "    \n",
    "    triplet_model = Model(inputs=[anchor_example, positive_example, negative_example],\n",
    "                  outputs=loss)\n",
    "    \n",
    "    triplet_model.compile(loss='mean_absolute_error', optimizer=Adam())\n",
    "    print(triplet_model.summary())\n",
    "    \n",
    "    return triplet_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor (InputLayer)             (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 62, 62, 64)   19392       anchor[0][0]                     \n",
      "                                                                 positive[0][0]                   \n",
      "                                                                 negative[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 64)           0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 64)           0           sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 64)           0           sequential_1[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 1)            0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_max_pooling2d_3[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 19,392\n",
      "Trainable params: 19,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Applications/anaconda/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "triplet_model=create_model(128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import SVG\n",
    "#from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "#SVG(model_to_dot(triplet_model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15180182456970215\n"
     ]
    }
   ],
   "source": [
    "triplet_str=[]\n",
    "\n",
    "t1= time.time()\n",
    "for i in range(1,13):\n",
    "    for ja in range(1,6):\n",
    "        for k in range(23,52):\n",
    "             for jn in range(1,6):\n",
    "                    for jp in range(1,6):\n",
    "                        #print (i,ja,k,jn,jp)\n",
    "                        if (jp!=ja):\n",
    "                            \n",
    "                            if(i<10):\n",
    "                                a='00'+ str(i)+'_00'+str(i)+'_00'+str(ja)\n",
    "                                b='00'+ str(i) +'_00'+str(i)+'_00'+str(jp)\n",
    "                                c='0'+str(k)+'_00'+str(i)+'_00'+str(jn)\n",
    "                            else:\n",
    "                                a='0'+str(i)+'_0'+str(i)+'_00'+str(ja)\n",
    "                                b='0'+str(i)+'_0'+str(i)+'_00'+str(jp)\n",
    "                                c='0'+str(k)+'_0'+str(i)+'_00'+str(jn)\n",
    "                            \n",
    "                            x= (a,b,c)\n",
    "                            triplet_str.append(x)\n",
    "                        \n",
    "                    \n",
    "t2=time.time()\n",
    "\n",
    "print (t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_array(image):\n",
    "    img = cv2.resize(image, (128, 128)) \n",
    "    x_train = np.array([img])\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_path_to_array(image_path):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    return img_to_array(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_database():\n",
    "    database = {}\n",
    "\n",
    "    # load all the images of individuals to recognize into the database\n",
    "    for file in glob.glob(\"images/*\"):\n",
    "        identity = os.path.splitext(os.path.basename(file))[0].strip('NISDCC-_6g')\n",
    "        #print (identity)\n",
    "        database[identity] = img_path_to_array(file)\n",
    "\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_arrays = prepare_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor=[]\n",
    "positive=[]\n",
    "negative=[]\n",
    "\n",
    "for img_name in triplet_str:\n",
    "    anchor.append(train_image_arrays[img_name[0]])\n",
    "    positive.append(train_image_arrays[img_name[1]])\n",
    "    negative.append(train_image_arrays[img_name[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.reshape(anchor,(len(anchor),128,128,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a=np.array(anchor).reshape((len(anchor), 128,128,3))\n",
    "x_p=np.array(positive).reshape((len(positive), 128,128,3))\n",
    "x_n=np.array(negative).reshape((len(negative), 128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAH6FJREFUeJzt3XmYlXX9//HnWxZNRfZwBA1+CHkp\naeRompYkZm6XYi5RpqQWluaehhnatzSX3JdUMpXURMUNFRFERCglZ0QElQFyY9gcEBBBVj+/P875\nnDn3zJmZs933fc7welyXlzP3ue9zv7k5fM77s5tzDhERb5u4AxCR0qJCQUQCVCiISIAKBREJUKEg\nIgEqFEQkQIWCiASEViiY2RFmVmNmC8xsRFj3EZHisjAGL5lZG2Ae8AOgFngD+Ilz7t2i30xEiqpt\nSO+7P7DAOfc+gJmNAY4DMhYK3bp1c7179w4pFBEBqK6uXu6c697SeWEVCj2BhWm/1wLfTj/BzIYD\nwwF22203qqqqQgpFRADM7KNszoutodE5N8o5V+mcq+zevcXCS0QiElahsAjYNe33XsljIlLiwioU\n3gD6mVkfM2sPDAXGhXQvESmiUNoUnHObzew3wItAG+A+59w7YdxLRIorrIZGnHPjgfFhvb+IhEMj\nGkUkQIWCiASoUBCRABUKIhKgQkFEAlQoiEiACgURCVChICIBKhREJECFgogEqFAQkQAVCiISoEJB\nRAJUKIhIgAoFEQlQoSAiASoURCRAhYKIBKhQEJEAFQoiEqBCQUQCVCiISIAKBREJUKEgIgEqFEQk\nIO9Cwcx2NbMpZvaumb1jZucnj3cxs0lmNj/5/87FC1dEwlZIprAZuNg5tydwAHCOme0JjAAmO+f6\nAZOTv4tImci7UHDOLXHOvZn8eQ3wHtATOA4YnTxtNDCk0CBFJDpFaVMws97AQGAG0MM5tyT50lKg\nRxPXDDezKjOrqqurK0YYIlIEBRcKZrYj8ARwgXPus/TXnHMOcJmuc86Ncs5VOucqu3fvXmgYIlIk\nBRUKZtaORIHwsHPuyeThZWZWkXy9AviksBCjc88993DPPffEHYZIrArpfTDgH8B7zrmb0l4aBwxL\n/jwMeCb/8EQkam0LuPYg4FRgtpm9lTz2e+Ba4DEzOxP4CDi5sBCjs3Tp0rhDEIld3oWCc246YE28\nPDjf9xWReBWSKbQaK1asAGC77baLORKR+GmYs4gEKFMAnnkm0Rbapk2bmCOR1mzy5MkMHlz6NWsV\nCsD69esBGDBgQMyRSGv04YcfAvDKK6+URaGg6oOIBChToL4kP/vss+MNRFql5557DoA99tgj5kiy\no0xBRAJaXaZw+OGHM3HixKzPf/bZZzn66KNDjEi2Vi+99BIAPXok5gSedNJJcYaTtVZXKPzgBz9g\nypQpAHz/+99v8fwVK1bQpUuXsMOSrdA777wDwPnnnx9zJLlR9UFEAlpdprDTTjsxa9YsILtMYdGi\nRey8885hhyVbmbfffpvddtst7jDyokxBRAJaXaZQXV1Nr169sjoP4PLLL+fqq68G4Igjjgg1Ntl6\nTJ06lXPPPTfuMPLS6gqFjRs3ZjWx6fnnnwdg3333ZfPmzWGHJVuJxYsXA2T1xVSqVH0QkYBWlyk4\n59hrr72afN1nBemNizvssEPoccnW4dlnnwXgrLPOijmS/ClTEJGAVpcptG3blsMOOwyA2267DYDz\nzjsv9foNN9wAwIgR9XvU9O3bN8IIpTVKLFwOa9eujTmSwilTEJGAVpcpbNiwgW233RaA1atXB16b\nM2cO69atCxxbsmQJxx9/fGTxSevktwa46KKLYo6kcK2mUNi4cSMAPXv2TB3zDYg33ZRYgX7x4sVc\ne+21geuWLVtGRUVFRFFKa7VmzZq4QygaVR9EJKDVZAozZ84EYM8990wd86ncz372MyAxUKlt2+Af\n+a233uKb3/xmRFFKa/P4448DcMkll8QcSfEoUxCRgIIzBTNrA1QBi5xzx5hZH2AM0BWoBk51zm0s\n9D4tefPNNwH43ve+1+i1hx56qMnrSnFXqLFjxwJw4oknxhyJtKQUPz+FKkb14XzgPWCn5O/XATc7\n58aY2d3AmcBdRbhPs/yKzP3798/pui+//DKMcPLix1UMGzashTOlFHzwwQd07Ngx7jCKrtBdp3sB\nRwP3Jn834FBgbPKU0cCQQu4hItEqNFO4BbgU6JD8vSuwyjnnpx3WAj0zXVhsvkuoXbt2OV23YcOG\nMMLJ2dVXX82pp54K0Cq/fVqj1157jdNOOy3uMIqukK3ojwE+cc5V53n9cDOrMrOqurq6fMMQkSIr\ndCv6Y83sKGA7Em0KtwKdzKxtMlvoBSzKdLFzbhQwCqCystIVEAdQP3ip3Nxyyy0AHHXUUWW7fNfW\nqra2Nu4QQpF3puCcu8w518s51xsYCrzsnDsFmAL4ZvNhwDMFRykikQlj8NLvgDFmdhUwE/hHCPdo\npOGchmzFlWE88cQTALRv3x6AgQMHxhKH5M7v53DhhRfGHEk4ilIoOOdeAV5J/vw+sH8x3jcXXbt2\nzeu6bJZuK7b777+fmpoagEZzMaT0vffeewCpKfqtjUY0ikhAq5n74KdL56pbt25FjqRpd999NwB1\ndXXKEMpYa5oRmYkyBREJKPtM4eWXX87rui+++ALIPFeiWB5++GEAOnRIjO1avnw5ACNHjgztnhKe\nF154ASi/vSFzVfaFgp/zkOvIRF8oDBgwoOgxARx99NGpDUYvuOACILHxjJSvefPmAXDkkUc2eY7v\nVTrhhBMiiSkMqj6ISEDZZwqffvopkPveDdOnTwfg2GOPLXpMAN27d09tG3bQQQcBkJgvJuWqTZs2\nLZ4zd+7cCCIJlzIFEQko+0xhyZIlAHTp0iWn6z7//PMwwkkZOXJkqg663377hXovCd/cuXNTy/pl\nMnXqVAAOOeSQqEIKTdkXCn6RlK985Ss5Xef/wYalb9++2mSmFZk1axZ77LFHo+P+8+e3I2wNhYKq\nDyISUPaZgi+h995775yua7iqs0hzVqxYkfH4woULAZg/fz4AgwcPjiymsChTEJGAsv+69Muv5ToI\nSZmC5OKzzz5rdGzLli2ptqlf/epXUYcUGmUKIhJQ9l+X+S68Wq7Lt0k8KisrGx0bM2ZMVgOayk3Z\nFwr5/uMulVWcpbTNmTMHgIMPPjh17JNPPgESIxx32WWXWOIKk6oPIhJQ9plCroOWPL82okhzevXq\nBQSX7XvrrbeARENjmFPv46JMQUQCyj5T2LRpU17XldIeklK6OnXqBMDtt9/O4sWLATjjjDMA+O9/\n/xtbXGEq+0Ih123iPD8SUiQbixcvTk3P99WHIUOi2ybVLyYUxerjqj6ISEBZZwpr1qzJu/pwzDHH\nFDkaac2uueaa1M8333wzACeddFJk97/nnnuAaNaHVKYgIgEFZQpm1gm4FxgAOOAMoAZ4FOgNfAic\n7JxbWVCUTd+fLVu25HVtrouyiPjFfuPozvYrgUP9rmIVFRUADBs2rKj3KrT6cCswwTl3opm1B7YH\nfg9Mds5da2YjgBEk9pcsutmzZ+fd8PL1r3+9yNFsvR555BGgvuEt37Ejpc4v2X/OOedEds/nn38e\ngEsvvRSAe++9l7q6OoBUb0ixC4W8qw9m1hH4HskNZJ1zG51zq4DjgNHJ00YD0TXRikjBCskU+gB1\nwP1mtg9QDZwP9HDOLUmesxToUViITXv//fdz3i7OL4qx6667hhFSznxKWq7frn/9619T62T6b66L\nL744zpBC4+c8ROmjjz4C6jcUWrRoETfeeCMAv/tdIgH/7W9/yw033FC0exbS0NgW+BZwl3NuILCW\nRFUhxTnnSLQ1NGJmw82sysyqfDokIvErJFOoBWqdczOSv48lUSgsM7MK59wSM6sAMhavzrlRwCiA\nysrKjAVHS7bZZhs6duyY0zXNZQr//ve/gfp9GqJw/fXXA3DllVfm/R5PP/00AO+88w677bYbAKee\nemrgnBkzZvDtb38773s0NHbsWCDxDXbJJZcA8Pvf/75o719qNm3axI477hj5ff229zNmJP6Z+cZF\ngOuuuw6g6JsV550pOOeWAgvNzLfYDQbeBcYBvuVjGPBMQRGKSKQK7X04F3g42fPwPnA6iYLmMTM7\nE/gIOLnAezRp1apVqVls2fLDRTPxS2tFmSkUstjLrbfeCsCyZcsAWLlyJf/73/+AxplCsevDfnmy\n9GXIfPtIa/TUU09x3nnnFfw+ubRpPfnkk3Tt2hWAt99+G4Dhw4c3Os+3OxRLQYWCc+4toPGSNIms\nIXQrV65k6NChOV3jt5lL50dFJppAWrZgwQIAdt9995zune6VV14BoGfPnnldP3Xq1NQHzMdx1VVX\n8fOf/xyo//D4giPTykGFyLQHQr7zUKZNm8Z3v/vdQkMKVbEK1UcffRRINA62pKamJvWZXLp0aZPn\n5bplYks0olFEAsp67kP79u3p3LlzTtcsWrSo0THfmNOvX7+s3uOxxx4DCmtY86V7VVVVXtfPnDmT\nU045BYCBAwemjv/zn/8E6rurRo4cCVDULiuA73znO42OpY+6y8bo0YnhLB07duS1114D4MADDyw8\nuCLys2nzHTmbzs+ZyNaXX36Zql7utNNOjV73XcG+u7JYlCmISEBZZwr5rImQ6Rr/LZBpbf9MMjWo\nvfDCCwAceeSRWb3HSy+9BJBzpvOf//wHSNQx0zOEhvy3iJnl9P6F2HnnnbM6z9eTn3vuOQAef/zx\n1CzEUssUfIxnnXVW3u/h26zmz5+fVbfmE088AcC5557Ln/70JyDzgDDfplRId3YmZV0oZPuPOF2m\nJbnnzp0LZLdBzB133ME+++zT6LivBqQXCtXV1QDsu+++gXNnzZrFZZddBsBf/vKXLCNPePnll4H6\nsfBN8ZN2vvrVr+b0/vnwPR4XXXRRVuf/61//Aur/DDfddFPRU+Bi8Q18+cyxue+++4D6AvrGG29k\n+vTpLV6XvoJ0c0vIN9eTVghVH0QkoKwzhc8//zznazJlA9tskygbs1k0Y8mSJfzmN78JHNuwYUPG\nKkVTw7enTZuWyjayHeK9atUqoL5LtaWp3/fee29W71sMfpxE3759mzzHj9dfvXp16pvT96+vWLGC\nbt26hRxlftatW5f3tR9//DFQP5bj8ccf57TTTmvyfJ/5+rEJn3/+ebPdvD5DK/aK0soURCSgrDOF\nfAaUNNwZav369TkN/sj0jTZp0qSMXVYNGzV9VpBeF8y2IfDBBx8EYMSIES2cGT1fB87UTekzBN/Y\ntsMOO6S+Ef3zufHGG4s+fr9QfnHW448/Pq/r77zzTv74xz8GjrXUbuIbNX0m+thjj2X8fKxevRqg\n2YbmQihTEJGAss4U8tlOvuG39+LFi7OqN/oux0GDBqXe4+9//zuQ6NFomG18+umnqdmLfpHYP/zh\nD0D9IpwA3bt3zypuX2+PojchV00NWnLOpZ6tH0Q1evTo1FDfdH4thlIxf/58IP/FWX17QrqW5ob4\nthZv9erVGRcm9r1lxZz1mq6sC4V8NnTZa6+9Ar+/9tprWU2A8v/oBw4cmPqAr1yZWHryjjvuSI3O\n82pqalLdWH6ce6ZqyooVK1q897PPPlvSy8c19ffw4IMPpgpCz6e+6Z588sm854CExY8ByNVdd90F\nBMcO+EKzqQbBBx54AIALLrggcHzz5s0ZR9lm+0WSL1UfRCSgrDOFXJdig/rUy6f0y5cvT02Z7t27\nd5PXpTdq+u3H//znP6eO+W4k/6356KOPpuZG+NF6t99+e15/htmzZ5f0Aia+S7eh7t27pxrK/Bj+\nE044odF5H330Ef379y96XOlVkly3jPddwLnymdD222/fKI6999474zW+itBwoFJtbS3HHnts4NjH\nH38ceO8wKFMQkYCyzhSKUQ9ds2ZNxrUBPN9NdOKJJ6aO/frXv250nm+Y8l2GV1xxRWqAUaYMwQ88\n+eUvf9lijJnq4aVi/fr1TbbJpM9I9bM3f/GLXzQ6r6Kiotm/g1z5rMTfM595C7lmoc88k1hgLFOX\nsW9AzJQpVFdX841vfCPjey5btqzRkPqFCxeGvghQWRYKvuHG7/6bi4YjxNasWZNa1zCTTFNWM6mt\nrQVg7dq1QMsjDn1vQqa+fW/ChAlA5gVNSsX48eP50Y9+FDj2+uuvA8ECIFMPz/vvvw8kCoVipsR+\nYZl8xnT4z9bpp5+e03XN9Z74P2cmGzdubHISWLt27VJVM18tnTdvXuiFgqoPIhJQlpmC7y7KZ0RX\nw+6cTP3JUF8dyHa1aD/bMdu002cKzfEzLxt265WSTN2RmeakZMoUfAPv4MGDGTduHABf+9rX8o5l\n5syZAPTp0yfv9/CzUE8+ObulRSdOnAg0Xw3s1KlTo2NvvPEG0Pzcivbt26dmu44fPz6nuAqhTEFE\nAsoyU/DfCPlkCg27HZsaPDRt2jQg+3aLXBumGo5eyySf9SKi9sEHHzQ6VlNTA8Bhhx2WOpZp9Kmf\nh9KuXTtmz54NZO6y9Pzf+6uvvpr6hvXrUqxduzY1gOyWW27J+c/h5bIy8ubNm1Nd3IcffniT52Vq\nU/jwww+B5kdMps/T8VnVUUcdlXV8+SrLQmHNmjV5X+sb9nzDYKZFLKZPn97s6rnF0Fy1xK+uVOwV\nmMOQ/sH1/e2ZFv9I//P6gsQ3ykLzk9v838VTTz0FJBpx/T/ehx56CEiM5SjGRqvNLWrS0PTp07Na\n9j29iuXXytxvv/1avG79+vWpiXDHHXdc1nEVStUHEQkoy0whn8VVGvLfMMOGDUtNcT3zzDMBePHF\nFwOjFcPQ3Dfj5MmTgfpJRKUs/VvQj9y84oorGp2Xvt2Zn5b805/+NHXMjwhtaMqUKbz44ouB991+\n++1Tk6r8/hmrV68uylTibCbH+arOnDlzGDRoUIvn+wl011xzTSrDyWbfBzNLxVNI42mulCmISEBB\nmYKZXQj8gsTO0rNJbBtXAYwBupLYnv5U51z+e6NlUIxt2/0owaFDh3L55ZcD9QuC5DtdNhfNDYoq\n1+3XfONZJn7UXnV1NW+++SYQXMDEZ3/+78Kv+PzFF1+w//77A8H5BD/+8Y8BUvtF+MVgC5XNrFWf\nyTWc1dgUP4+iTZs23H333VnHsnbt2oJWkc5X3pmCmfUEzgMqnXMDgDbAUOA64Gbn3O7ASuDMYgQq\nItEotE2hLfAVM9sEbA8sAQ4FfGVxNPBH4K4C7xNQjDYFX/8FuPrqq4H6ZcUGDBhQ8Pu3pLm1IEpx\nIZWm9OjRg6uuugpofhl034q+fPnyjF2GfvckP6/EDxMfMmQIhxxySJPvW0j3YyZ+sFAmt912G5B7\nr5Afdp2rsNdNaErehYJzbpGZ3QB8DHwBTCRRXVjlnPPLG9UCRV89I59NYLIRRWHgZer6+tvf/gZk\nn5aWgrPOOis1lsPvc5CJX8OypTkOfpGSuPhqiy8A/DyDSZMmpaaBH3zwwZHEEtcK14VUHzoDxwF9\ngF2AHYAjcrh+uJlVmVlVtsuci0j4Cqk+HAZ84JyrAzCzJ4GDgE5m1jaZLfQCGu/oCjjnRgGjACor\nK7PbAz7Jb6VVrpYvX55xGm2xtjuPWnMZglfsrc3Ccv311wP1Myz9KMlOnTpF/rlrqps2bIV0SX4M\nHGBm21sirxoMvAtMAfziA8OAZwoLUUSiVEibwgwzGwu8CWwGZpL45n8eGGNmVyWP/aMYgbYm06ZN\ny7ifQLl2RbZGPrPx7Qj57CVZqGI0qOejoN4H59yVQMO88H1g/0Let7XLNBmqpqaGQw89NIZoJJNi\njIUpVxrRKCIBZTn3odxlalB8/vnns97KXbYOzY2ZCJMyBREJUKEQg0zbi2cz5l62LtluPlxsqj7E\nINPmKfnsiymtk1/+P6yRuy1RpiAiAfp6ioGfBwDw0ksvAeU130HC1bdvX6CwZQcLoUxBRAKUKcQg\nfTamX/8/feVj2br5Jdia2rg3bMoURCRAmUKE/CIuQ4YMSR2Lq4VZSpff3yKO+RagQiFSr776KhDt\nYi5SfnxhENeEKFUfRCRAmUKE/ArSUL9duxoYpSG/fmdcA9qUKYhIgDKFCKV3MfnGJL96sYg3depU\nIL4uSRUKEUrfUDWX3Y1l69KhQwdAcx9EpEQoU4hQXJt7SHnxU6bTM8soKVMQkQBlChHyuw/NmjWL\nH/7whzFHI6Vq48bEfswbNmyI5f4qFCLkR6pNmTJFU6WlSb6BMdMKXVFQ9UFEApQpRGDlypUADBo0\nCICnn346xmik1PnqQ1yUKYhIQIuFgpndZ2afmNmctGNdzGySmc1P/r9z8riZ2W1mtsDM3jazb4UZ\nfLmYMGECEyZMoH///vTv3z+2ZbakPHTo0IEOHToElu2LUjaZwgM03mJ+BDDZOdcPmJz8HeBIoF/y\nv+HAXcUJU0Si0mKh4Jx7Ffi0weHjgNHJn0cDQ9KO/9MlvE5iW/qKYgVbrlatWsWqVatSv1dUbPWP\nRJqxZcsWtmzZUnZdkj2cc36X1KVAj+TPPYGFaefVJo813lF1K+J3k544cSIAZ599dpzhSInzhUHv\n3r1juX/BDY0uMSLH5XqdmQ03syozq6qrqys0DBEpknwLhWW+WpD8v98xdRGwa9p5vZLHGnHOjXLO\nVTrnKlv7nIB169axbt06unbtSteuXeMOR0qcr262adMmlvvnWyiMA4Ylfx4GPJN2/LRkL8QBwOq0\naoaIlIEW2xTM7BFgENDNzGqBK4FrgcfM7EzgI+Dk5OnjgaOABcA64PQQYi47fjDKvHnzANh3333j\nDEdKnN+C/pBDDonl/i0WCs65nzTx0uAM5zrgnEKDam38mntxtSZLefGfl7iWeNeIRhEJ0NyHCPTr\n1w+AU045JeZIpBz4Bkb/uYmaMgURCVCmEIH169cD8a3jL+XFf17iokxBRAL01RWi8ePHA2jAkuRk\n06ZNsd5fhUKIFi5MTAPZe++9Y45EykncO5Gr+iAiAcoUQuQHK8U1hl0kH8oURCRAmUKI/IKtcQ1C\nkfK00047xXp/FQoh8hOhOnfuHHMkUk7imvPgqfogIgHKFEIUd9eSlKe4Z9MqUxCRAGUKIdpxxx3j\nDkHKUNwZpgqFEI0cOTLuEKQMdevWLdb7q/ogIgHKFERKjJnFen9lCiISoExBpMT4kbBxUaYgIgHK\nFERKzNChQ2O9vwoFkRLTv3//WO+v6oOIBLRYKJjZfWb2iZnNSTv2VzOba2Zvm9lTZtYp7bXLzGyB\nmdWY2Q/DClxEwpFNpvAAcESDY5OAAc65vYF5wGUAZrYnMBTYK3nN38xMyw6JlJEWCwXn3KvApw2O\nTXTO+QHar5PYch7gOGCMc26Dc+4DEhvN7l/EeEUkZMVoUzgDeCH5c09gYdprtcljIlImCioUzOxy\nYDPwcB7XDjezKjOrqqurKyQMESmivAsFM/s5cAxwSnILeoBFwK5pp/VKHmvEOTfKOVfpnKvs3r17\nvmGISJHlVSiY2RHApcCxzrl1aS+NA4aa2bZm1gfoB/y38DBFJCotDl4ys0eAQUA3M6sFriTR27At\nMCk5o+t159yvnHPvmNljwLskqhXnOOe2hBW8iBSf1Wf+8amsrHRVVVVxhyHSqplZtXOusqXzNKJR\nRAJUKIhIgAoFEQlQoSAiASoURCRAhYKIBKhQEJEAFQoiElASg5fMrA5YCyyPOxagG4ojneIIKuc4\nvuaca3GiUUkUCgBmVpXNaCvFoTgUR7hxqPogIgEqFEQkoJQKhVFxB5CkOIIUR1Crj6Nk2hREpDSU\nUqYgIiWgJAoFMzsiuU/EAjMbEdE9dzWzKWb2rpm9Y2bnJ493MbNJZjY/+f/OEcXTxsxmmtlzyd/7\nmNmM5DN51MzaRxBDJzMbm9zT4z0zOzCO52FmFyb/TuaY2SNmtl1Uz6OJfU4yPgNLuC0Z09tm9q2Q\n44hkv5XYC4XkvhB3AkcCewI/Se4fEbbNwMXOuT2BA4BzkvcdAUx2zvUDJid/j8L5wHtpv18H3Oyc\n2x1YCZwZQQy3AhOcc3sA+yTjifR5mFlP4Dyg0jk3AGhDYi+RqJ7HAzTe56SpZ3AkiSUH+wHDgbtC\njiOa/Vacc7H+BxwIvJj2+2XAZTHE8QzwA6AGqEgeqwBqIrh3LxIftkOB5wAjMTClbaZnFFIMHYEP\nSLYzpR2P9HlQv01AFxLLBT4H/DDK5wH0Bua09AyAe4CfZDovjDgavHY88HDy58C/GeBF4MB87xt7\npkAJ7BVhZr2BgcAMoIdzbknypaVAjwhCuIXEQrhfJn/vCqxy9RvuRPFM+gB1wP3Jasy9ZrYDET8P\n59wi4AbgY2AJsBqoJvrnka6pZxDnZze0/VZKoVCIlZntCDwBXOCc+yz9NZcodkPtnjGzY4BPnHPV\nYd4nC22BbwF3OecGkhh2HqgqRPQ8OpPYaawPsAuwA43T6NhE8QxaUsh+K9kohUIh670iis3M2pEo\nEB52zj2ZPLzMzCqSr1cAn4QcxkHAsWb2ITCGRBXiVqCTmfnVtqN4JrVArXNuRvL3sSQKiaifx2HA\nB865OufcJuBJEs8o6ueRrqlnEPlnt9D9VrJRCoXCG0C/ZOtyexINJuPCvqkl1qb/B/Cec+6mtJfG\nAcOSPw8j0dYQGufcZc65Xs653iT+7C87504BpgAnRhjHUmChmX09eWgwiaX6I30eJKoNB5jZ9sm/\nIx9HpM+jgaaewTjgtGQvxAHA6rRqRtFFtt9KmI1GOTSoHEWiNfV/wOUR3fNgEmng28Bbyf+OIlGf\nnwzMB14CukT4HAYBzyV//n/Jv9gFwOPAthHc/5tAVfKZPA10juN5AP8HzAXmAA+S2GMkkucBPEKi\nLWMTiezpzKaeAYkG4TuTn9vZJHpMwoxjAYm2A/95vTvt/MuTcdQARxZyb41oFJGAUqg+iEgJUaEg\nIgEqFEQkQIWCiASoUBCRABUKIhKgQkFEAlQoiEjA/wfr/Ul7/iehBwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12af799b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(x_a[100])\n",
    "plt.show()\n",
    "x_a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def generator():\n",
    "    dataset=load_images(\"images/*\")\n",
    "\n",
    "    anchor=[]\n",
    "    p_img=[]\n",
    "    n_img=[]\n",
    "    \n",
    "    for i,j,k in dataset:\n",
    "        anchor.append(i)\n",
    "        p_img.append(j)\n",
    "        n_img.append(k)\n",
    "    \n",
    "    anchor=np.array(anchor)\n",
    "    p_img=np.array(p_img)\n",
    "    n_img=np.array(n_img)\n",
    "    \n",
    "    return anchor, p_img, n_img\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "\r",
      " 1024/34800 [..............................] - ETA: 7:50:54 - loss: 43.7301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 2048/34800 [>.............................] - ETA: 6:30:40 - loss: 31.6988\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 3072/34800 [=>............................] - ETA: 5:28:05 - loss: 25.2297\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 4096/34800 [==>...........................] - ETA: 5:15:53 - loss: 20.5641\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 5120/34800 [===>..........................] - ETA: 4:46:38 - loss: 17.5607\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 6144/34800 [====>.........................] - ETA: 4:30:33 - loss: 15.3377\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 7168/34800 [=====>........................] - ETA: 4:33:58 - loss: 13.5123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 8192/34800 [======>.......................] - ETA: 4:20:57 - loss: 12.0557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      " 9216/34800 [======>.......................] - ETA: 4:15:21 - loss: 10.8796\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "10240/34800 [=======>......................] - ETA: 4:05:41 - loss: 9.8754 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "11264/34800 [========>.....................] - ETA: 3:55:17 - loss: 9.0823\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "12288/34800 [=========>....................] - ETA: 3:45:01 - loss: 8.3719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "13312/34800 [==========>...................] - ETA: 3:36:13 - loss: 7.8078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "14336/34800 [===========>..................] - ETA: 3:26:27 - loss: 7.2731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "15360/34800 [============>.................] - ETA: 3:16:58 - loss: 6.8264\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "16384/34800 [=============>................] - ETA: 3:07:19 - loss: 6.4145\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "17408/34800 [==============>...............] - ETA: 2:56:30 - loss: 6.0614\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "18432/34800 [==============>...............] - ETA: 2:48:18 - loss: 5.7620\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "19456/34800 [===============>..............] - ETA: 2:37:05 - loss: 5.4840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "20480/34800 [================>.............] - ETA: 2:27:23 - loss: 5.2524\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "21504/34800 [=================>............] - ETA: 2:16:01 - loss: 5.0164\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "22528/34800 [==================>...........] - ETA: 2:31:25 - loss: 4.7911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "23552/34800 [===================>..........] - ETA: 2:17:41 - loss: 4.5929\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "24576/34800 [====================>.........] - ETA: 2:03:03 - loss: 4.4215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "25600/34800 [=====================>........] - ETA: 1:50:00 - loss: 4.2491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "26624/34800 [=====================>........] - ETA: 1:36:45 - loss: 4.1004\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "27648/34800 [======================>.......] - ETA: 1:23:38 - loss: 3.9485\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "28672/34800 [=======================>......] - ETA: 1:10:53 - loss: 3.8157\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "29696/34800 [========================>.....] - ETA: 58:13 - loss: 3.6968  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "30720/34800 [=========================>....] - ETA: 46:25 - loss: 3.5786\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "31744/34800 [==========================>...] - ETA: 34:33 - loss: 3.4635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "32768/34800 [===========================>..] - ETA: 22:51 - loss: 3.3564\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "33792/34800 [============================>.] - ETA: 11:17 - loss: 3.2547\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "34800/34800 [==============================] - 23283s 669ms/step - loss: 3.1624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12b007ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplet_model.fit(x=[x_a, x_p, x_n], y=np.zeros(x_a.shape[0]), verbose=1, batch_size=1024, epochs=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
