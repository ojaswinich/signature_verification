{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D, concatenate, Activation\n",
    "from keras.applications.mobilenet import MobileNet as Net\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "#from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import time\n",
    "import glob\n",
    "\n",
    "ALPHA = 0.7 # Triplet Loss Parameter\n",
    "\n",
    "from keras.layers import Input,Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D,concatenate,Activation\n",
    "from keras.applications.xception import Xception as Net\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random as rn\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(3)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the triplet loss to be used in cnn model\n",
    "\n",
    "def triplet_loss(x):\n",
    "    anchor, positive, negative = x\n",
    "    \n",
    "    #x = tf.constant([[1, 1, 1], [1, 1, 1]]) ------ tf.reduce_sum(x, 1)  # [3, 3]\n",
    "    #It is trained in batches so, the tensor will be of the above shape for each triplet\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), 1)\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), 1)\n",
    "    \n",
    "    #x = tf.constant([[1, 1, 1], [1, 1, 1]])--------->tf.reduce_sum(x, 0)  # [2, 2, 2]\n",
    "\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), ALPHA)\n",
    "    loss = tf.reduce_mean(tf.maximum(basic_loss, 0.0), 0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_to_array(image):\n",
    "    img = cv2.resize(image, (128, 128)) \n",
    "    x_train = np.array([img])\n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_path_to_array(image_path):\n",
    "    img1 = cv2.imread(image_path, 1)\n",
    "    return img_to_array(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_database_tr(location):\n",
    "    database = {}\n",
    "\n",
    "    # load all the images of individuals to recognize into the database\n",
    "    for file in glob.glob(location):\n",
    "        identity = os.path.splitext(os.path.basename(file))[0].strip('NISDCC-_6g')\n",
    "        #print (identity)\n",
    "        database[identity] = img_path_to_array(file)\n",
    "\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image_arrays = prepare_database_tr('/Users/ochhab3/Documents/ojaswinich_github/signature_verification/images/sigcomp2009/training/*.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15772533416748047\n"
     ]
    }
   ],
   "source": [
    "triplet_str_tr=[]\n",
    "\n",
    "t1= time.time()\n",
    "for i in range(1,13):\n",
    "    for ja in range(1,6):\n",
    "        for k in range(23,52):\n",
    "             for jn in range(1,6):\n",
    "                    for jp in range(1,6):\n",
    "                        #print (i,ja,k,jn,jp)\n",
    "                        if (jp!=ja):\n",
    "                            \n",
    "                            if(i<10):\n",
    "                                a='00'+ str(i)+'_00'+str(i)+'_00'+str(ja)\n",
    "                                b='00'+ str(i) +'_00'+str(i)+'_00'+str(jp)\n",
    "                                c='0'+str(k)+'_00'+str(i)+'_00'+str(jn)\n",
    "                            else:\n",
    "                                a='0'+str(i)+'_0'+str(i)+'_00'+str(ja)\n",
    "                                b='0'+str(i)+'_0'+str(i)+'_00'+str(jp)\n",
    "                                c='0'+str(k)+'_0'+str(i)+'_00'+str(jn)\n",
    "                            \n",
    "                            x= (a,b,c)\n",
    "                            triplet_str_tr.append(x)\n",
    "                        \n",
    "                    \n",
    "t2=time.time()\n",
    "\n",
    "print (t2-t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchor_tr=[]\n",
    "positive_tr=[]\n",
    "negative_tr=[]\n",
    "\n",
    "for img_name in triplet_str_tr:\n",
    "    anchor_tr.append(train_image_arrays[img_name[0]])\n",
    "    positive_tr.append(train_image_arrays[img_name[1]])\n",
    "    negative_tr.append(train_image_arrays[img_name[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_a_tr=np.array(anchor_tr).reshape((len(anchor_tr), 128,128,3))\n",
    "x_p_tr=np.array(positive_tr).reshape((len(positive_tr), 128,128,3))\n",
    "x_n_tr=np.array(negative_tr).reshape((len(negative_tr), 128,128,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34800"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_test_dict(location):\n",
    "    database = {}\n",
    "\n",
    "    # load all the images of individuals to recognize into the database\n",
    "    for file in glob.glob(location):\n",
    "        identity = os.path.splitext(os.path.basename(file))[0].strip('NFI-')\n",
    "        #print (identity)\n",
    "        database[identity] = img_path_to_array(file)\n",
    "\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_genuine_dict = prepare_test_dict('/Users/ochhab3/Documents/ojaswinich_github/signature_verification/images/sigcomp2009/test/genuines/*')\n",
    "test_forgery_dict = prepare_test_dict('/Users/ochhab3/Documents/ojaswinich_github/signature_verification/images/sigcomp2009/test/forgeries/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ids = np.arange(1, 101)\n",
    "missing_ids = np.array([5, 13, 25, 32, 34, 36, 38, 40, 48, 50, 52, 57, 60, 61, 65, 76, 78, 81, 82, 87, 95])\n",
    "ids_available = np.array(list((set(all_ids)-set(missing_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j_formatted=[]\n",
    "for i in range(1,13):\n",
    "    if (i<10):\n",
    "        new_j='0'+str(i)\n",
    "    else:\n",
    "        new_j=str(i)\n",
    "    j_formatted.append(new_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_formatted=[]\n",
    "for i in ids_available:\n",
    "    if (i<10):\n",
    "        new_i='00'+str(i)\n",
    "    elif(9<i<100):\n",
    "        new_i='0'+str(i)\n",
    "    else:\n",
    "        new_i= str(i)\n",
    "    i_formatted.append(new_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_formatted=[]\n",
    "for i in range(3,101):\n",
    "    if (i<10):\n",
    "        new_f='00'+str(i)\n",
    "    elif(9<i<100):\n",
    "        new_f='0'+str(i)\n",
    "    else:\n",
    "        new_f= str(i)\n",
    "    f_formatted.append(new_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triplet_test_comb=[]\n",
    "for i in i_formatted:\n",
    "    for f in f_formatted:\n",
    "        for ja in j_formatted:\n",
    "            for jn in j_formatted[0:6]:\n",
    "                for jp in j_formatted:\n",
    "                    if (jp!=ja):\n",
    "                        a=i+ja+i\n",
    "                        p=i+jp+i\n",
    "                        n=f+jn+i\n",
    "                        triplet_test=(a,p,n)\n",
    "                        triplet_test_comb.append(triplet_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_triplet_test_id=[]\n",
    "for i in range(0,len(triplet_test_comb)):\n",
    "    if ((triplet_test_comb[i][0] in test_genuine_dict.keys()) and (triplet_test_comb[i][1] in test_genuine_dict.keys())and (triplet_test_comb[i][2] in test_forgery_dict.keys())):\n",
    "        final_triplet_test_id.append(triplet_test_comb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset test data triplets for training of cnn\n",
    "import random\n",
    "random.seed(0)\n",
    "p_test=random.sample(range(1, len(final_triplet_test_id)), 45000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_test_id=[]\n",
    "for i in p_test:\n",
    "    new_test_id.append(final_triplet_test_id[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchor_test=[]\n",
    "positive_test=[]\n",
    "negative_test=[]\n",
    "\n",
    "for img_name in new_test_id:\n",
    "    anchor_test.append(test_genuine_dict[img_name[0]])\n",
    "    positive_test.append(test_genuine_dict[img_name[1]])\n",
    "    negative_test.append(test_forgery_dict[img_name[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_a_test=np.array(anchor_test).reshape((len(anchor_test), 128,128,3))\n",
    "x_p_test=np.array(positive_test).reshape((len(positive_test), 128,128,3))\n",
    "x_n_test=np.array(negative_test).reshape((len(negative_test), 128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_dut_dict(location):\n",
    "    database = {}\n",
    "\n",
    "    # load all the images of individuals to recognize into the database\n",
    "    for file in glob.glob(location):\n",
    "        identity = os.path.splitext(os.path.basename(file))[0].strip('K')\n",
    "        #print (identity)\n",
    "        database[identity] = img_path_to_array(file)\n",
    "\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dut_all_dict = prepare_dut_dict('/Users/ochhab3/Documents/ojaswinich_github/signature_verification/images/dutch_updated/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dut_neg_dict={}\n",
    "\n",
    "for i in dut_all_dict.keys():\n",
    "    if len(i)==10 :\n",
    "        dut_neg_dict[i]= dut_all_dict[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "internal_count_ids = list(range(17, 70))\n",
    "internal_count_ids.append(13)\n",
    "internal_count_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "internal_count_dict = {}\n",
    "for i in range(0, len(internal_count_ids)):\n",
    "    internal_count_dict['0'+str(internal_count_ids[i])]=1\n",
    "\n",
    "dut_neg_dict_ord = {}\n",
    "\n",
    "for i in dut_neg_dict.keys():\n",
    "    person_id=i[-3:]\n",
    "    image_id=internal_count_dict[person_id]\n",
    "    #print(i, person_id, image_id)\n",
    "    if image_id<10:\n",
    "        dut_neg_dict_ord[person_id+'_0'+str(image_id)]= dut_neg_dict[i]\n",
    "    else:\n",
    "        dut_neg_dict_ord[person_id+'_'+str(image_id)]= dut_neg_dict[i]\n",
    "    internal_count_dict[person_id]=internal_count_dict[person_id]+1\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dut_i_formatted = ['013']\n",
    "\n",
    "for i in range(17, 70):\n",
    "    dut_i_formatted.append('0'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dut_j_formatted = []\n",
    "\n",
    "for j in range(1,25):\n",
    "    if (j<10):\n",
    "        new_j='0'+str(j)\n",
    "    else:\n",
    "        new_j=str(j)\n",
    "    dut_j_formatted.append(new_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "triplet_dut_comb=[]\n",
    "for i in dut_i_formatted:\n",
    "        for ja in dut_j_formatted:\n",
    "            for jn in dut_j_formatted[0:20]:\n",
    "                for jp in dut_j_formatted:\n",
    "                    if (jp!=ja):\n",
    "                        a=ja+'_'+i\n",
    "                        p=jp+'_'+i\n",
    "                        n=i+'_'+jn\n",
    "                        triplet_dut=(a,p,n)\n",
    "                        triplet_dut_comb.append(triplet_dut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_triplet_dut_id=[]\n",
    "for i in range(0,len(triplet_dut_comb)):\n",
    "    if ((triplet_dut_comb[i][0] in dut_all_dict.keys()) and (triplet_dut_comb[i][1] in dut_all_dict.keys())and (triplet_dut_comb[i][2] in dut_neg_dict_ord.keys())):\n",
    "        final_triplet_dut_id.append(triplet_dut_comb[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset dutch data triplets for training of cnn\n",
    "import random\n",
    "random.seed(0)\n",
    "p_dut=random.sample(range(1, len(final_triplet_dut_id)), 40000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_dutch_id=[]\n",
    "for i in p_dut:\n",
    "    new_dutch_id.append(final_triplet_dut_id[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anchor_dut=[]\n",
    "positive_dut=[]\n",
    "negative_dut=[]\n",
    "\n",
    "for img_name in new_dutch_id:\n",
    "    anchor_dut.append(dut_all_dict[img_name[0]])\n",
    "    positive_dut.append(dut_all_dict[img_name[1]])\n",
    "    negative_dut.append(dut_neg_dict_ord[img_name[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_a_dut=np.array(anchor_dut).reshape((len(anchor_dut), 128,128,3))\n",
    "x_p_dut=np.array(positive_dut).reshape((len(positive_dut), 128,128,3))\n",
    "x_n_dut=np.array(negative_dut).reshape((len(negative_dut), 128,128,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anchor_dut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running triplet model on train, test, and dutch\n",
    "\n",
    "x_a_comb= np.vstack((x_a_tr,x_a_test,x_a_dut))\n",
    "x_p_comb= np.vstack((x_p_tr,x_p_test,x_p_dut))\n",
    "x_n_comb= np.vstack((x_n_tr,x_n_test,x_n_dut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "pretrained_model= keras.applications.mobilenet.MobileNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1, 1, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_preds (Conv2D)          (None, 1, 1, 1000)        1025000   \n",
      "_________________________________________________________________\n",
      "act_softmax (Activation)     (None, 1, 1, 1000)        0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1000)              0         \n",
      "=================================================================\n",
      "Total params: 4,253,864\n",
      "Trainable params: 4,231,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_needed=pretrained_model.layers[0:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the Sequential model\n",
    "baseline_model=Sequential()\n",
    "for layer in layers_needed:\n",
    "    baseline_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers_to_not_train= baseline_model.layers[0:79]\n",
    "\n",
    "for layer in layers_to_not_train:\n",
    "    layer.trainable=False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 7, 7, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 1,050,624\n",
      "Non-trainable params: 2,178,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model_new(d1, d2, c):\n",
    "    \n",
    "    # The triplet network takes 3 input images: 2 of the same class and 1 out-of-class sample\n",
    "    #shape: A shape tuple (integers), not including the batch size. For instance, shape=(32,) \n",
    "    #indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "    \n",
    "    anchor_example =   Input(shape=(d1, d2, c), name = 'anchor')\n",
    "    positive_example = Input(shape=(d1, d2, c), name = 'positive')\n",
    "    negative_example = Input(shape=(d1, d2, c), name = 'negative')\n",
    "        \n",
    "    base_model = baseline_model\n",
    "    # the weights of this layer will be set to ones and fixed  (since they\n",
    "    # are shared we could also leave them trainable to get a weighted sum)\n",
    "    \n",
    "    # feed all 3 inputs into the pretrained keras model\n",
    "    x1 = base_model(anchor_example)\n",
    "    x2 = base_model(positive_example)\n",
    "    x3 = base_model(negative_example)\n",
    "    \n",
    "    # flatten/summarize the models output:\n",
    "    # (here we could also use GlobalAveragePooling or simply Flatten everything)\n",
    "    #Takes the max value for each of the filter\n",
    "    anchor =   GlobalMaxPooling2D()(x1)\n",
    "    positive = GlobalMaxPooling2D()(x2)\n",
    "    negative = GlobalMaxPooling2D()(x3)\n",
    "    \n",
    "    \n",
    "    #Loss gives the final loss value between a,p and n.\n",
    "    loss = merge([anchor, positive, negative], mode=triplet_loss, output_shape=(1,))\n",
    "\n",
    "    triplet_model = Model(inputs=[anchor_example, positive_example, negative_example],\n",
    "                  outputs=loss)\n",
    "    \n",
    "    triplet_model.compile(loss='mean_absolute_error', optimizer=Adam())\n",
    "    print(triplet_model.summary())\n",
    "    \n",
    "    return triplet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor (InputLayer)             (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative (InputLayer)           (None, 128, 128, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       multiple             3228864     anchor[0][0]                     \n",
      "                                                                 positive[0][0]                   \n",
      "                                                                 negative[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_1 (GlobalM (None, 1024)         0           sequential_1[1][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_2 (GlobalM (None, 1024)         0           sequential_1[2][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling2d_3 (GlobalM (None, 1024)         0           sequential_1[3][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 1)            0           global_max_pooling2d_1[0][0]     \n",
      "                                                                 global_max_pooling2d_2[0][0]     \n",
      "                                                                 global_max_pooling2d_3[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 1,050,624\n",
      "Non-trainable params: 2,178,240\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Applications/anaconda/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "triplet_model=create_model_new(128, 128, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triplet_model.fit(x=[x_a_comb, x_p_comb, x_n_comb], y=np.zeros(x_a_comb.shape[0]), verbose=1, batch_size=2048, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model trained above\n",
    "triple_model_path = '/Users/ochhab3/Documents/ojaswinich_github/signature_verification/model_weights/triplet_weights_new_e3.h5'\n",
    "# triplet_model.save_weights(triple_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the weights of pre-trained final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model.load_weights(triple_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the embeddings from the anchors\n",
    "intermediate_layer_model_anchor = Model(inputs=triplet_model.input,\n",
    "                                 outputs=triplet_model.get_layer('global_max_pooling2d_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the embeddings from the positives\n",
    "intermediate_layer_model_positive = Model(inputs=triplet_model.input,\n",
    "                                 outputs=triplet_model.get_layer('global_max_pooling2d_2').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find the embeddings from the negatives\n",
    "intermediate_layer_model_negative = Model(inputs=triplet_model.input,\n",
    "                                 outputs=triplet_model.get_layer('global_max_pooling2d_3').output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anc_emb_tr=intermediate_layer_model_anchor.predict([x_a_tr, x_p_tr, x_n_tr])\n",
    "pos_emb_tr=intermediate_layer_model_positive.predict([x_a_tr, x_p_tr, x_n_tr])\n",
    "neg_emb_tr=intermediate_layer_model_negative.predict([x_a_tr, x_p_tr, x_n_tr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anc_emb_te=intermediate_layer_model_anchor.predict([x_a_test, x_p_test,x_n_test])\n",
    "pos_emb_te=intermediate_layer_model_positive.predict([x_a_test, x_p_test,x_n_test])\n",
    "neg_emb_te=intermediate_layer_model_negative.predict([x_a_test, x_p_test,x_n_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anc_emb_dut=intermediate_layer_model_anchor.predict([x_a_dut, x_p_dut,x_n_dut])\n",
    "pos_emb_dut=intermediate_layer_model_positive.predict([x_a_dut, x_p_dut,x_n_dut])\n",
    "neg_emb_dut=intermediate_layer_model_negative.predict([x_a_dut, x_p_dut,x_n_dut])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Embedding Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Unique Identification for training set\n",
    "\n",
    "data_anc_tr = {}\n",
    "data_pos_tr = {}\n",
    "data_neg_tr = {}\n",
    "for i in range(0,len(triplet_str_tr)):\n",
    "    #Anchor dict\n",
    "    a_id = triplet_str_tr[i][0]\n",
    "    if a_id not in data_anc_tr.keys():\n",
    "        data_anc_tr[a_id]= np.array(anc_emb_tr[i])\n",
    "        \n",
    "    #Pos dict\n",
    "    p_id = triplet_str_tr[i][1]\n",
    "    if p_id not in data_pos_tr.keys():\n",
    "        data_pos_tr[p_id]= np.array(pos_emb_tr[i])\n",
    "        \n",
    "    #Neg dict\n",
    "    n_id = triplet_str_tr[i][2]\n",
    "    if n_id not in data_neg_tr.keys():\n",
    "        data_neg_tr[n_id]= np.array(neg_emb_tr[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Embedding Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Unique Identification for test set embedding\n",
    "\n",
    "data_anc_te = {}\n",
    "data_pos_te = {}\n",
    "data_neg_te = {}\n",
    "for i in range(0,len(new_test_id)):\n",
    "    #Anchor dict\n",
    "    a_id = new_test_id[i][0]\n",
    "    if a_id not in data_anc_te.keys():\n",
    "        data_anc_te[a_id]= np.array(anc_emb_te[i])\n",
    "        \n",
    "    #Pos dict\n",
    "    p_id = new_test_id[i][1]\n",
    "    if p_id not in data_pos_te.keys():\n",
    "        data_pos_te[p_id]= np.array(pos_emb_te[i])\n",
    "        \n",
    "    #Neg dict\n",
    "    n_id = new_test_id[i][2]\n",
    "    if n_id not in data_neg_te.keys():\n",
    "        data_neg_te[n_id]= np.array(neg_emb_te[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch Data Embedding Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating Unique Identification for dutch data set embedding\n",
    "\n",
    "data_anc_dut = {}\n",
    "data_pos_dut = {}\n",
    "data_neg_dut = {}\n",
    "for i in range(0,len(new_dutch_id)):\n",
    "    #Anchor dict\n",
    "    a_id = new_dutch_id[i][0]\n",
    "    if a_id not in data_anc_dut.keys():\n",
    "        data_anc_dut[a_id]= np.array(anc_emb_dut[i])\n",
    "        \n",
    "    #Pos dict\n",
    "    p_id = new_dutch_id[i][1]\n",
    "    if p_id not in data_pos_dut.keys():\n",
    "        data_pos_dut[p_id]= np.array(pos_emb_dut[i])\n",
    "        \n",
    "    #Neg dict\n",
    "    n_id = new_dutch_id[i][2]\n",
    "    if n_id not in data_neg_dut.keys():\n",
    "        data_neg_dut[n_id]= np.array(neg_emb_dut[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pairwise pos str\n",
    "pair_pos_str_tr=[]\n",
    "for i in range(1,13):\n",
    "    for ja in range(1,6):\n",
    "        for jp in range(1,6):\n",
    "            if (jp!=ja):\n",
    "                if (i<10) :\n",
    "                    a='00'+ str(i)+'_00'+str(i)+'_00'+str(ja)\n",
    "                    b='00'+ str(i) +'_00'+str(i)+'_00'+str(jp)\n",
    "                else :\n",
    "                   a='0'+str(i)+'_0'+str(i)+'_00'+str(ja)\n",
    "                   b='0'+str(i)+'_0'+str(i)+'_00'+str(jp)\n",
    "                \n",
    "                pos_pair=(a,b)\n",
    "                pair_pos_str_tr.append(pos_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pairwise negative str\n",
    "pair_neg_str_tr=[]\n",
    "for i in range(1,13):\n",
    "    for ja in range(1,6):\n",
    "        for k in range(23,52):\n",
    "            for jn in range(1,6):\n",
    "                if (i<10):\n",
    "                    a='00'+ str(i)+'_00'+str(i)+'_00'+str(ja)\n",
    "                    b='0'+ str(k) +'_00'+str(i)+'_00'+str(jn)\n",
    "                else :\n",
    "                    a='0'+ str(i)+'_0'+str(i)+'_00'+str(ja)\n",
    "                    b='0'+ str(k)+'_0'+str(i)+'_00'+str(jn)\n",
    "                    \n",
    "                pair_neg=(a,b)\n",
    "                pair_neg_str_tr.append(pair_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortd_pair_pos_str_tr=[]\n",
    "for i in range(0,len(pair_pos_str_tr)):\n",
    "    x=np.sort(pair_pos_str_tr[i])\n",
    "    sortd_pair_pos_str_tr.append(x)\n",
    "    \n",
    "import pandas as pd\n",
    "sortd_pair_pos_str_tr=pd.DataFrame(sortd_pair_pos_str_tr)\n",
    "\n",
    "final_sortd_pair_pos_str_tr=sortd_pair_pos_str_tr.drop_duplicates()\n",
    "\n",
    "list_pos_tr=[]\n",
    "for i in range(0,len(final_sortd_pair_pos_str_tr)):\n",
    "    x=[final_sortd_pair_pos_str_tr.iloc[i][0],final_sortd_pair_pos_str_tr.iloc[i][1]]\n",
    "    list_pos_tr.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortd_pair_neg_str_tr=[]\n",
    "for i in range(0,len(pair_neg_str_tr)):\n",
    "    x=np.sort(pair_neg_str_tr[i])\n",
    "    sortd_pair_neg_str_tr.append(x)\n",
    "    \n",
    "sortd_pair_neg_str_tr=pd.DataFrame(sortd_pair_neg_str_tr)\n",
    "\n",
    "final_sortd_pair_neg_str_tr=sortd_pair_neg_str_tr.drop_duplicates()\n",
    "\n",
    "list_neg_tr=[]\n",
    "for i in range(0,len(final_sortd_pair_neg_str_tr)):\n",
    "    x=[final_sortd_pair_neg_str_tr.iloc[i][0],final_sortd_pair_neg_str_tr.iloc[i][1]]\n",
    "    list_neg_tr.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pairwise pos str\n",
    "pair_pos_str_te=[]\n",
    "for i in i_formatted:\n",
    "    for ja in j_formatted:\n",
    "        for jp in j_formatted:\n",
    "            if (jp!=ja):\n",
    "                a=i+ja+i\n",
    "                p=i+jp+i\n",
    "                pos_pair=(a,p)\n",
    "                pair_pos_str_te.append(pos_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairwise negative str\n",
    "pair_neg_str_te=[]\n",
    "for i in i_formatted:\n",
    "    for f in f_formatted:\n",
    "        for ja in j_formatted:\n",
    "            for jn in j_formatted[0:6]:\n",
    "                a=i+ja+i\n",
    "                n=f+jn+i\n",
    "                pair_neg=(a,n)\n",
    "                pair_neg_str_te.append(pair_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortd_pair_pos_str_te=[]\n",
    "for i in range(0,len(pair_pos_str_te)):\n",
    "    x=np.sort(pair_pos_str_te[i])\n",
    "    sortd_pair_pos_str_te.append(x)\n",
    "\n",
    "sortd_pair_pos_str_te=pd.DataFrame(sortd_pair_pos_str_te)\n",
    "\n",
    "final_sortd_pair_pos_str_te=sortd_pair_pos_str_te.drop_duplicates()\n",
    "\n",
    "list_pos_te=[]\n",
    "for i in range(0,len(final_sortd_pair_pos_str_te)):\n",
    "    x=[final_sortd_pair_pos_str_te.iloc[i][0],final_sortd_pair_pos_str_te.iloc[i][1]]\n",
    "    list_pos_te.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortd_pair_neg_str_te=[]\n",
    "for i in range(0,len(pair_neg_str_te)):\n",
    "    x=np.sort(pair_neg_str_te[i])\n",
    "    sortd_pair_neg_str_te.append(x)\n",
    "    \n",
    "sortd_pair_neg_str_te=pd.DataFrame(sortd_pair_neg_str_te)\n",
    "\n",
    "final_sortd_pair_neg_str_te=sortd_pair_neg_str_te.drop_duplicates()\n",
    "\n",
    "list_neg_te=[]\n",
    "for i in range(0,len(final_sortd_pair_neg_str_te)):\n",
    "    x=[final_sortd_pair_neg_str_te.iloc[i][0],final_sortd_pair_neg_str_te.iloc[i][1]]\n",
    "    list_neg_te.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only getting the combinations that are present in the embedding dictionary\n",
    "\n",
    "list_pos_te_fn=[]\n",
    "for i in list_pos_te:\n",
    "    if (i[0] in data_anc_te.keys() and i[1] in data_pos_te.keys()):\n",
    "        list_pos_te_fn.append(i)\n",
    "\n",
    "list_neg_te_fn=[]\n",
    "for i in list_neg_te:\n",
    "    if (i[0] in data_anc_te.keys() and i[1] in data_neg_te.keys()):\n",
    "        list_neg_te_fn.append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch Data Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_pos_str_dut=[]\n",
    "\n",
    "for i in dut_i_formatted:\n",
    "    for ja in dut_j_formatted:\n",
    "        for jp in dut_j_formatted:\n",
    "            if (jp!=ja):\n",
    "                a=ja+'_'+i\n",
    "                p=jp+'_'+i\n",
    "                pair_dut=(a,p)\n",
    "                pair_pos_str_dut.append(pair_dut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_neg_str_dut=[]\n",
    "\n",
    "for i in dut_i_formatted:\n",
    "    for ja in dut_j_formatted:\n",
    "        for jn in dut_j_formatted[0:20]:\n",
    "                a=ja+'_'+i\n",
    "                n=i+'_'+jn\n",
    "                pair_dut=(a,n)\n",
    "                pair_neg_str_dut.append(pair_dut)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortd_pair_pos_str_dut=[]\n",
    "for i in range(0,len(pair_pos_str_dut)):\n",
    "    x=np.sort(pair_pos_str_dut[i])\n",
    "    sortd_pair_pos_str_dut.append(x)\n",
    "    \n",
    "sortd_pair_pos_str_dut=pd.DataFrame(sortd_pair_pos_str_dut)  \n",
    "final_sortd_pair_pos_str_dut=sortd_pair_pos_str_dut.drop_duplicates()\n",
    "\n",
    "list_pos_dt=[]\n",
    "for i in range(0,len(final_sortd_pair_pos_str_dut)):\n",
    "    x=[final_sortd_pair_pos_str_dut.iloc[i][0],final_sortd_pair_pos_str_dut.iloc[i][1]]\n",
    "    list_pos_dt.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortd_pair_neg_str_dut=[]\n",
    "for i in range(0,len(pair_neg_str_dut)):\n",
    "    x=np.sort(pair_neg_str_dut[i])\n",
    "    sortd_pair_neg_str_dut.append(x)\n",
    "    \n",
    "sortd_pair_neg_str_dut=pd.DataFrame(sortd_pair_neg_str_dut)\n",
    "final_sortd_pair_neg_str_dut=sortd_pair_neg_str_dut.drop_duplicates()\n",
    "\n",
    "list_neg_dt=[]\n",
    "for i in range(0,len(final_sortd_pair_neg_str_dut)):\n",
    "    x=[final_sortd_pair_neg_str_dut.iloc[i][0],final_sortd_pair_neg_str_dut.iloc[i][1]]\n",
    "    list_neg_dt.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only getting the combinations that are present in the embedding dictionary\n",
    "list_pos_dt_fn=[]\n",
    "for i in list_pos_dt:\n",
    "    if (i[0] in data_anc_dut.keys() and i[1] in data_pos_dut.keys()):\n",
    "        list_pos_dt_fn.append(i)\n",
    "\n",
    "list_neg_dt_fn=[]\n",
    "for i in list_neg_dt:\n",
    "    if (i[0] in data_anc_dut.keys() and i[1] in data_neg_dut.keys()):\n",
    "        list_neg_dt_fn.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Positive Features\n",
    "pos_feat_tr=np.zeros((len(list_pos_tr),1024))\n",
    "for i in range(0,len(list_pos_tr)):\n",
    "    x1=data_anc_tr[list_pos_tr[i][0]]\n",
    "    x2=data_anc_tr[list_pos_tr[i][1]]\n",
    "    pos_feat_tr[i]=x1-x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Negative Features\n",
    "neg_feat_tr=np.zeros((len(list_neg_tr),1024))\n",
    "for i in range(0,len(list_neg_tr)):\n",
    "    x1=data_anc_tr[list_neg_tr[i][0]]\n",
    "    x2=data_neg_tr[list_neg_tr[i][1]]\n",
    "    neg_feat_tr[i]=x1-x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1024) (8700, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(pos_feat_tr.shape, neg_feat_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Features\n",
    "pos_feat_te=np.zeros((len(list_pos_te_fn),1024))\n",
    "for i in range(0,len(list_pos_te_fn)):\n",
    "    x1=data_anc_te[list_pos_te_fn[i][0]]\n",
    "    x2=data_pos_te[list_pos_te_fn[i][1]]\n",
    "    pos_feat_te[i]=x1-x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Features\n",
    "neg_feat_te=np.zeros((len(list_neg_te_fn),1024))\n",
    "for i in range(0,len(list_neg_te_fn)):\n",
    "    x1=data_anc_te[list_neg_te_fn[i][0]]\n",
    "    x2=data_neg_te[list_neg_te_fn[i][1]]\n",
    "    neg_feat_te[i]=x1-x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1203, 1024) (2016, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(pos_feat_te.shape, neg_feat_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dutch Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive Features\n",
    "pos_feat_dut=np.zeros((len(list_pos_dt_fn),1024))\n",
    "for i in range(0,len(list_pos_dt_fn)):\n",
    "    x1=data_anc_dut[list_pos_dt_fn[i][0]]\n",
    "    x2=data_pos_dut[list_pos_dt_fn[i][1]]\n",
    "    pos_feat_dut[i]=x1-x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Negative Features\n",
    "neg_feat_dut=np.zeros((len(list_neg_dt_fn),1024))\n",
    "for i in range(0,len(list_neg_dt_fn)):\n",
    "    x1=data_anc_dut[list_neg_dt_fn[i][0]]\n",
    "    x2=data_neg_dut[list_neg_dt_fn[i][1]]\n",
    "    neg_feat_dut[i]=x1-x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14858, 1024) (1755, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(pos_feat_dut.shape, neg_feat_dut.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16181"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total positive pairs\n",
    "14858+1203+120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12471"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total negative pairs\n",
    "8700+2016+1755"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMBINE TEST TRAIN FEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_feat_total=np.vstack((pos_feat_tr,pos_feat_te,pos_feat_dut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_feat_total=np.vstack((neg_feat_tr,neg_feat_te, neg_feat_dut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_total=np.vstack((pos_feat_total,neg_feat_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genuine as 0 and Forged as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_Y = [0]*len(pos_feat_total)\n",
    "#makes that no of ones as the len of the item given\n",
    "neg_Y = [1]*len(neg_feat_total)\n",
    "#rowwise appends the two arrays\n",
    "train_y=np.append(pos_Y, neg_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#training on 80% data and test on remaining 20% data\n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(feat_total, train_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(random_state=0)\n",
    "log_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9849483006849614"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "log_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9654510556621881"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "log_model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967558799675588\n",
      "0.9528753993610224\n"
     ]
    }
   ],
   "source": [
    "# precision recall on test data\n",
    "print (recall_score(y_test, log_model.predict(x_test)))\n",
    "print (precision_score(y_test, log_model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3147,  118],\n",
       "       [  80, 2386]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=log_model.predict(x_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96685565, 0.95377235, 0.95551679, 0.95769734, 0.96249455,\n",
       "       0.96247818, 0.95853339, 0.96377128, 0.95635094, 0.95591445])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(log_model, x_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95837463, 0.94350842, 0.94373149, 0.94752475, 0.95073892,\n",
       "       0.957     , 0.94669299, 0.95083579, 0.94204322, 0.93429952])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(log_model, x_train, y_train, cv=10,scoring='precision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96603397, 0.95104895, 0.95504496, 0.95604396, 0.96403596,\n",
       "       0.957     , 0.959     , 0.967     , 0.959     , 0.967     ])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(log_model, x_train, y_train, cv=10,scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=300, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_model = RandomForestClassifier(n_estimators=500, max_depth=10,random_state=0)\n",
    " \n",
    "forest_model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9667117490510885"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train accuracy\n",
    "forest_model.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946257197696737"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy\n",
    "forest_model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8864557988645579\n",
      "0.987353206865402\n"
     ]
    }
   ],
   "source": [
    "# precision recall on test data\n",
    "print (recall_score(y_test, forest_model.predict(x_test)))\n",
    "print (precision_score(y_test, forest_model.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3237,   28],\n",
       "       [ 280, 2186]])"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=forest_model.predict(x_test)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the log model weights\n",
    "import pickle\n",
    "log_model_path = '/Users/ochhab3/Documents/ojaswinich_github/signature_verification/model_weights/log_model_e3.pkl'\n",
    "file_object=open(log_model_path, 'wb')\n",
    "s=pickle.dump(log_model, file_object)\n",
    "file_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance between Positive and Anchor ---> d_ap\n",
    "d_ap=[]\n",
    "for i in range(0,len(x_a_test)):\n",
    "    d_ap.append(np.linalg.norm(anc_emb_te[i]-pos_emb_te[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5ae0d0e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(d_ap)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance between Negative and Anchor ---> d_an\n",
    "d_an=[]\n",
    "for i in range(0,len(x_a_test)):\n",
    "    d_an.append(np.linalg.norm(anc_emb_te[i]-neg_emb_te[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5af19e6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(d_an)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance of distances ---> d_final\n",
    "d_final = []\n",
    "for i in range(0,len(d_ap)):\n",
    "    diff = d_ap[i]-d_an[i]\n",
    "    d_final.append(diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYzhIkjr/HyEqVng8kTKcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x5ad3d4390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(d_final)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
